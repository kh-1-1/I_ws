# 超参数配置与调优

<cite>
**本文档中引用的文件**  
- [train.py](file://AEMCARL/crowd_nav/train.py#L1-L250)
- [trainer.py](file://AEMCARL/crowd_nav/utils/trainer.py#L1-L86)
- [memory.py](file://AEMCARL/crowd_nav/utils/memory.py#L1-L28)
- [explorer.py](file://AEMCARL/crowd_nav/utils/explorer.py#L1-L158)
- [policy_factory.py](file://AEMCARL/crowd_nav/policy/policy_factory.py#L1-L19)
</cite>

## 目录
1. [引言](#引言)
2. [核心超参数列表](#核心超参数列表)
3. [超参数对训练过程的影响](#超参数对训练过程的影响)
4. [不同场景下的调参建议](#不同场景下的调参建议)
5. [常见问题诊断与解决方案](#常见问题诊断与解决方案)
6. [结论](#结论)

## 引言
本指南旨在系统性地分析CrowdNav项目中基于`train.py`的超参数配置，深入探讨各关键超参数对强化学习训练过程的影响。通过理解学习率、折扣因子、经验回放缓冲区大小等参数的作用机制，为研究人员和开发者提供一套完整的调参策略，以优化模型在不同人群密度环境下的导航性能。

**节来源**  
- [train.py](file://AEMCARL/crowd_nav/train.py#L1-L250)

## 核心超参数列表

以下是从`train.py`及其相关模块中提取的关键超参数，分为训练控制、探索策略、优化器配置和奖励设计四类。

### 训练控制参数
- **rl_learning_rate**：强化学习阶段的学习率，控制模型权重更新步长。
- **train_batches**：每轮训练中执行的优化批次数量。
- **train_episodes**：总的训练回合数。
- **sample_episodes**：每次采样并存入经验池的训练回合数。
- **target_update_interval**：目标网络更新频率（每隔多少episode更新一次）。
- **evaluation_interval**：模型评估间隔（每隔多少episode进行一次验证集评估）。
- **checkpoint_interval**：模型检查点保存间隔。

### 探索与经验回放参数
- **capacity**：经验回放缓冲区的最大容量。
- **epsilon_start**：初始探索率（ε-greedy策略起始值）。
- **epsilon_end**：最终探索率（ε-greedy策略终止值）。
- **epsilon_decay**：探索率线性衰减的周期（episode数）。

### 优化器配置
- **optimizer**：使用的优化器类型，支持SGD、Adam、RMSprop。
- **batch_size**：训练时每个批次的数据量。

### 奖励函数参数
- **agent_timestep**：智能体的时间步长。
- **human_timestep**：人类代理的时间步长。
- **reward_increment**：奖励增量系数。
- **position_variance**：位置方差参数，影响奖励计算。
- **direction_variance**：方向方差参数，影响奖励计算。

**节来源**  
- [train.py](file://AEMCARL/crowd_nav/train.py#L1-L250)
- [trainer.py](file://AEMCARL/crowd_nav/utils/trainer.py#L1-L86)
- [memory.py](file://AEMCARL/crowd_nav/utils/memory.py#L1-L28)

## 超参数对训练过程的影响

### 学习率（rl_learning_rate）
学习率决定了模型参数更新的速度。过高的学习率可能导致训练不稳定甚至发散；过低的学习率则会导致收敛速度缓慢。建议初始设置为1e-3（SGD）或5e-4（Adam），并在训练过程中根据损失曲线动态调整。

### 折扣因子（gamma）
虽然未在`train.py`中直接暴露为命令行参数，但`explorer.py`中使用了`gamma`来计算累积折扣奖励。较高的gamma值（接近1）使智能体更关注长期回报，适合复杂路径规划；较低的gamma值（如0.9）则更注重即时奖励，可能加快初期学习速度但影响最终性能。

### 经验回放缓冲区大小（capacity）
较大的缓冲区能存储更多多样化的经验，提升样本利用率和训练稳定性。然而，过大的缓冲区会增加内存消耗，并可能导致旧经验干扰当前策略学习。通常建议设置为10,000至50,000之间。

### 批量大小（batch_size）
批量大小影响梯度估计的稳定性。较大的batch size提供更稳定的梯度方向，但需要更多显存；小batch size具有更强的正则化效果，但可能导致训练波动。推荐值为32或64。

### 目标网络更新频率（target_update_interval）
目标网络用于稳定Q值估计。更新频率过高会削弱其稳定性作用，导致训练震荡；过低则会使目标滞后，延缓收敛。常见设置为每100个episode更新一次。

### 探索率衰减策略（epsilon_start, epsilon_end, epsilon_decay）
采用线性衰减策略，在训练初期保持高探索率（如0.9），逐步降至低值（如0.1）。合理的衰减周期应覆盖训练前期，确保充分探索环境。若衰减过快，可能导致欠探索；过慢则浪费计算资源。

**节来源**  
- [train.py](file://AEMCARL/crowd_nav/train.py#L1-L250)
- [explorer.py](file://AEMCARL/crowd_nav/utils/explorer.py#L1-L158)
- [trainer.py](file://AEMCARL/crowd_nav/utils/trainer.py#L1-L86)

## 不同场景下的调参建议

### 低密度人群场景
- **建议**：可适当提高学习率（如1e-3），加快收敛速度。
- **探索策略**：使用较短的epsilon_decay（如500 episodes），快速收敛到最优策略。
- **经验回放**：capacity可设为较小值（如10,000），因状态空间较小。
- **目标网络更新**：可设为每50轮更新一次，加速训练。

### 高密度人群场景
- **建议**：降低学习率（如5e-4），避免因奖励稀疏导致的训练不稳定。
- **探索策略**：延长epsilon_decay至2000+ episodes，确保充分探索拥挤环境中的可行路径。
- **经验回放**：增大capacity至50,000以上，以覆盖更多复杂交互模式。
- **优化器选择**：优先使用Adam优化器，因其自适应学习率特性更适合非平稳环境。

### 动态障碍物频繁出现的场景
- **奖励设计**：适当调高`position_variance`和`direction_variance`，增强对避障行为的奖励敏感度。
- **时间步长**：减小`agent_timestep`以提高响应频率。
- **评估频率**：提高`evaluation_interval`，密切监控模型在动态环境中的泛化能力。

**节来源**  
- [train.py](file://AEMCARL/crowd_nav/train.py#L1-L250)
- [explorer.py](file://AEMCARL/crowd_nav/utils/explorer.py#L1-L158)

## 常见问题诊断与解决方案

### 过拟合现象
- **症状**：训练成功率持续上升，但验证/测试成功率停滞或下降。
- **诊断方法**：
  - 检查`success_rate`与`collision_rate`在训练集与验证集上的差异。
  - 观察`cumulative_rewards`是否在训练集上显著高于验证集。
- **解决方案**：
  - 增加经验回放缓冲区的随机采样强度（可通过增加batch_size间接实现）。
  - 引入行为克隆（Imitation Learning）预训练，提供更稳健的初始策略。
  - 使用dropout或L2正则化（需修改模型结构）。

### 训练发散
- **症状**：损失函数剧烈震荡或持续上升，成功率无明显提升。
- **诊断方法**：
  - 查看`Average loss`日志是否呈指数级增长。
  - 检查是否频繁发生碰撞或超时。
- **解决方案**：
  - 降低学习率至少一个数量级。
  - 减小gamma值（如从0.99降至0.95），减少长期预测误差累积。
  - 确保目标网络按时更新，防止Q值过高估计。
  - 使用梯度裁剪（需在`trainer.py`中添加）。

### 探索不足（欠探索）
- **症状**：早期成功率增长缓慢，模型倾向于保守路径。
- **诊断方法**：
  - 检查`epsilon`衰减曲线是否过快。
  - 分析`min_dist`统计值是否普遍偏大。
- **解决方案**：
  - 延长`epsilon_decay`周期。
  - 提高`epsilon_start`至0.9以上。
  - 在奖励函数中加入探索鼓励项（如访问新区域奖励）。

### 收敛速度慢
- **诊断方法**：
  - 训练多轮后`success_rate`仍低于预期。
  - `average_epoch_loss`下降缓慢。
- **解决方案**：
  - 切换至Adam优化器。
  - 增加`sample_episodes`以丰富训练数据。
  - 启用多进程训练（通过`multi_process`参数控制）。

**节来源**  
- [train.py](file://AEMCARL/crowd_nav/train.py#L1-L250)
- [trainer.py](file://AEMCARL/crowd_nav/utils/trainer.py#L1-L86)
- [explorer.py](file://AEMCARL/crowd_nav/utils/explorer.py#L1-L158)

## 结论
合理配置超参数是成功训练CrowdNav模型的关键。应根据具体应用场景灵活调整学习率、探索策略和经验回放机制。建议采用“先预训练（IL），再微调（RL）”的两阶段策略，并结合日志监控与定期评估，及时发现并解决训练过程中的问题。未来可考虑引入自动化超参数搜索工具（如Optuna）进一步提升调参效率。